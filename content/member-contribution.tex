\section*{Member contributions}

\subsubsection*{Nils Krehl}
tbd

\subsubsection*{Felix Hausberger}

First, literature research was done on Q-learning itself, going over to approximative Q-learning, Deep Q-learning, double DQNs and dueling DQNs. Then further improvements were investigated by looking at the prioritized experience replay buffer concept, different exploration methods and the rainbow paper from Google DeepMind. Also Bomberman related papers and experience reports for Reinforcement Learning were analyzed, also for hyperparameter tuning. 

Then the Q-learning algorithm got implemented using both an online and target network. Responsibility was also taken for both the normal and the prioritized experience replay buffer including the SumTree data structure. After shaping the rewards, choosing the right exploration method and setting the hyperparameters based on the research results, the training of the agents was executed, visualized and analyzed. Possible improvements for the problems during the training phase were considered and applied. The reasons for the bad local optimum were analyzed. The experimental results, open problems and improvements were then summarized in the conclusion.
