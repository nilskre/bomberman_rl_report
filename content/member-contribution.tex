\section*{Member contributions}

\subsubsection*{Nils Krehl}

Literature research on related literature about Reinforcement learning approaches and the concrete application of Reinforcement learning for Bomberman was done. Further literature research was done for the area of cloud training and possibilities for speeding up the training in the cloud.

Different feature representations based on the findings of the literature research were implemented. For finding the best feature representation the first attempt was an empirical evaluation. Due to slow training speed, no meaningful results were achieved yet. Therefore the proposed best feature representation from the related literature was finally implemented.
As part of the network architecture the dueling optimization of the DQN and the graphical visualization of our approach was done.
Moreover training experiments with different hyperparameters were executed.
Furthermore the possibility of cloud training was evaluated.
Responsibility was taken for the submission test of our agent.

\subsubsection*{Felix Hausberger}

First, literature research was done on Q-learning itself, going over to approximative Q-learning, Deep Q-learning, double DQNs and dueling DQNs. Then further improvements were investigated by looking at the prioritized experience replay buffer concept, different exploration methods and the rainbow paper from Google DeepMind. Also Bomberman related papers and experience reports for Reinforcement Learning were analyzed, also for hyperparameter tuning. 

Then the Q-learning algorithm got implemented using both an online and target network. Responsibility was also taken for both the normal and the prioritized experience replay buffer including the SumTree data structure. After shaping the rewards, choosing the right exploration method and setting the hyperparameters based on the research results, the training of the agents was executed, visualized and analyzed. Possible improvements for the problems during the training phase were considered and applied. The reasons for the bad local optimum were analyzed. The experimental results, open problems and improvements were then summarized in the conclusion.
