
@inproceedings{Kormelink2018,
	title = {Exploration {Methods} for {Connectionist} {Q}-{Learning} in {Bomberman}},
	doi = {10.5220/0006556403550362},
	author = {Kormelink, Joseph Groot and Drugan, Madalina and Wiering, Marco},
	year = {2018},
}

@article{Franca2019,
	title = {Learning {How} to {Play} {Bomberman} with {Deep} {Reinforcement} and {Imitation} {Learning}},
	doi = {10.1007/978-3-030-34644-7_10},
	journal = {Entertainment Computing and Serious Games},
	author = {França, Ícaro Goulart Faria Motta and Paes, Aline and Clua, Esteban},
	year = {2019},
}

@article{Wang2016,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1511.06581},
	journal = {arXiv},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	year = {2016},
}

@phdthesis{daCruzLopes2016,
	address = {Departamento de Ciência de Computadores},
	title = {Bomberman as an {Artificial} {Intelligence} {Platform}},
	url = {https://repositorio-aberto.up.pt/bitstream/10216/91011/2/176444.pdf},
	school = {Universidade do Porto},
	author = {da Cruz Lopes, Manuel António},
	year = {2016},
}

@article{vanHasselt2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {https://arxiv.org/abs/1509.06461},
	journal = {arXiv},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	year = {2015},
}

@inproceedings{Huang2018,
	title = {V-{D} {D3QN}: the {Variant} of {Double} {Deep} {Q}-{Learning} {Network} with {Dueling} {Architecture}},
	doi = {10.23919/ChiCC.2018.8483478},
	author = {Huang, Ying and Wei, GuoLiang and Wang, YongXiong},
	year = {2018},
}

@book{Geron2018,
	title = {Praxiseinstieg {Machine} {Learning} mit {Scikit}-{Learn} und {TensorFlow}},
	isbn = {978-3-96006-061-8},
	publisher = {O'REILLEY},
	author = {Géron, Aurélien},
	year = {2018},
}

@article{Hessel2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1710.02298},
	journal = {arXiv},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	year = {2017},
}

@article{Schaul2016,
	title = {Prioritized {Experience} {Replay}},
	url = {https://arxiv.org/abs/1511.05952},
	journal = {arXiv},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	year = {2016},
}

@article{Silver1140,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
	volume = {362},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/362/6419/1140},
	doi = {10.1126/science.aar6404},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	number = {6419},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science
tex.eprint: https://science.sciencemag.org/content/362/6419/1140.full.pdf},
	pages = {1140--1144},
}

@article{Salvador2020,
	title = {Reinforcement learning: {A} literature review ({September} 2020)},
	doi = {10.13140/RG.2.2.30323.76327},
	author = {Salvador, José and Oliveira, João and Breternitz, Maurício},
	month = oct,
	year = {2020},
}

@misc{TechPowerUp,
	title = {{TechPowerUp}},
	url = {https://www.techpowerup.com/gpu-specs/},
	abstract = {Graphics card and GPU database with specifications for products launched in recent years. Includes clocks, photos, and technical details.},
	language = {en},
	urldate = {2021-03-26},
	journal = {TechPowerUp},
	author = {TechPowerUp (editor)},
}

@article{hester2017deep,
	title = {Deep q-learning from demonstrations},
	author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
	year = {2017},
	note = {arXiv: 1704.03732 [cs.AI]},
}

@misc{PreisRechner2021,
	title = {Google {Cloud} {Platform}-{Preisrechner}},
	url = {https://cloud.google.com/products/calculator?hl=de},
	abstract = {Erstellen Sie auf der Grundlage von Anzahl, Nutzung und Leistungsstärke der Server Ihr eigenes benutzerdefiniertes Preisangebot für die von der Google Cloud Platform angebotenen Produkte.},
	language = {de},
	urldate = {2021-03-24},
	author = {Google (editor)},
	file = {Snapshot:C\:\\Users\\D070497\\Zotero\\storage\\9W5DI2U6\\calculator.html:text/html},
}

@misc{GCP2021,
	title = {{GPUs} zum {Trainieren} von {Modellen} in der {Cloud} verwenden},
	url = {https://cloud.google.com/ai-platform/training/docs/using-gpus?hl=de},
	language = {de},
	urldate = {2021-03-24},
	journal = {Google Cloud},
	author = {Google (editor)},
	file = {Snapshot:C\:\\Users\\D070497\\Zotero\\storage\\CLWPA692\\using-gpus.html:text/html},
}

@misc{Nvidia,
	title = {{CUDA} {GPUs}},
	url = {https://developer.nvidia.com/cuda-gpus},
	abstract = {Recommended GPU for Developers NVIDIA TITAN RTX NVIDIA TITAN RTX is built for data science, AI research, content creation and general GPU development. Built on the Turing architecture, it features 4608, 576 full-speed mixed precision Tensor Cores for accelerating AI, and 72 RT cores for accelerating ray tracing. It also includes 24 GB of GPU memory for training neural networks with large batch sizes, processing big datasets and working with large animation models and other memory-intensive workflows.},
	language = {en},
	urldate = {2021-03-24},
	journal = {NVIDIA Developer},
	author = {Nvidia (editor)},
	month = jun,
	year = {2012},
	file = {Snapshot:C\:\\Users\\D070497\\Zotero\\storage\\KH46XL87\\cuda-gpus.html:text/html},
}

@misc{Colab2021,
	title = {Colaboratory – {Google}, {FAQ}},
	url = {https://research.google.com/colaboratory/faq.html},
	urldate = {2021-03-24},
	author = {Google (editor)},
	file = {Colaboratory – Google:C\:\\Users\\D070497\\Zotero\\storage\\PTLKV523\\faq.html:text/html},
}

@misc{Hale2019,
	title = {Best {Deals} in {Deep} {Learning} {Cloud} {Providers}},
	url = {https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a},
	abstract = {Where to train deep learning models online for the lowest cost and least hassle},
	language = {en},
	urldate = {2021-03-24},
	journal = {Medium},
	author = {Hale, Jeff},
	month = apr,
	year = {2019},
	file = {Snapshot:C\:\\Users\\D070497\\Zotero\\storage\\ZLLNFMFR\\maximize-your-gpu-dollars-a9133f4e546a.html:text/html},
}

@inproceedings{Lawrence2017ComparingTD,
	title = {Comparing {TensorFlow} deep learning performance using {CPUs}, {GPUs}, local {PCs} and cloud},
	author = {Lawrence, John and Malmsten, J. and Rybka, A. and Sabol, Danny and Triplin, Ken},
	year = {2017},
}

@misc{Irpan2018,
	title = {Deep {Reinforcement} {Learning} {Doesn}'t {Work} {Yet}},
	url = {https://www.alexirpan.com/2018/02/14/rl-hard.html},
	urldate = {2021-03-27},
	author = {Irpan, Alex},
	month = jun,
	year = {2018},
}
